@Article{2023_ElgueaAguinaco,
  author     = {{\'{I}}{\~{n}}igo Elguea-Aguinaco and Antonio Serrano-Mu{\~{n}}oz and Dimitrios Chrysostomou and Ibai Inziarte-Hidalgo and Simon B{\o}gh and Nestor Arana-Arexolaleiba},
  journal    = {Robotics and Computer-Integrated Manufacturing},
  title      = {{A} review on reinforcement learning for contact-rich robotic manipulation tasks},
  year       = {2023},
  month      = {jun},
  pages      = {102517},
  volume     = {81},
  abstract   = {Research and application of reinforcement learning in robotics for contact-rich manipulation tasks have exploded in recent years. Its ability to cope with unstructured environments and accomplish hard-to-engineer behaviors has led reinforcement learning agents to be increasingly applied in real-life scenarios. However, there is still a long way ahead for reinforcement learning to become a core element in industrial applications. 
This paper examines the landscape of reinforcement learning and reviews advances in its application in contact-rich tasks from 2017 to the present. The analysis investigates the main research for the most commonly selected tasks for testing reinforcement learning algorithms in both rigid and deformable object manipulation. Additionally, the trends around reinforcement learning associated with serial manipulators are explored as well as the various technological challenges that this machine learning control technique currently presents. Lastly, based on the state-of-the-art and the commonalities among the studies, a framework relating the main concepts of reinforcement learning in contact-rich manipulation tasks is proposed. 
The final goal of this review is to support the robotics community in future development of systems commanded by reinforcement learning, discuss the main challenges of this technology and suggest future research directions in the domain.},
  doi        = {10.1016/j.rcim.2022.102517},
  file       = {:files/2023_ElgueaAguinaco.pdf:PDF;1st and 2nd pass review:notes/2023_ElgueaAguinaco.odt:OpenDocument text},
  publisher  = {Elsevier {BV}},
  readstatus = {skimmed},
  url        = {https://www.sciencedirect.com/science/article/pii/S0736584522001995},
}

@Article{2021_Lobbezoo,
  author     = {Andrew Lobbezoo and Yanjun Qian and Hyock-Ju Kwon},
  journal    = {Robotics},
  title      = {{R}einforcement {L}earning for {P}ick and {P}lace {O}perations in {R}obotics: {A} {S}urvey},
  year       = {2021},
  month      = {sep},
  number     = {3},
  pages      = {105},
  volume     = {10},
  abstract   = {The field of robotics has been rapidly developing in recent years, and the work related to
training robotic agents with reinforcement learning has been a major focus of research. This survey
reviews the application of reinforcement learning for pick-and-place operations, a task that a logistics
robot can be trained to complete without support from a robotics engineer. To introduce this topic, we
first review the fundamentals of reinforcement learning and various methods of policy optimization,
such as value iteration and policy search. Next, factors which have an impact on the pick-and-place
task, such as reward shaping, imitation learning, pose estimation, and simulation environment are
examined. Following the review of the fundamentals and key factors for reinforcement learning,
we present an extensive review of all methods implemented by researchers in the field to date.
The strengths and weaknesses of each method from literature are discussed, and details about the
contribution of each manuscript to the field are reviewed. The concluding critical discussion of the
available literature, and the summary of open problems indicates that experiment validation, model
generalization, and grasp pose selection are topics that require additional research.},
  doi        = {10.3390/robotics10030105},
  file       = {:files/2021_Lobbezoo.pdf:PDF;:notes/2021_Lobbezoo.odt:OpenDocument text},
  publisher  = {{MDPI} {AG}},
  readstatus = {skimmed},
  url        = {https://www.mdpi.com/2218-6581/10/3/105},
}

@Article{2014_Goodfellow,
  author        = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title         = {{G}enerative {A}dversarial {N}etworks},
  year          = {2014},
  month         = jun,
  abstract      = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1406.2661},
  eprint        = {1406.2661},
  file          = {:files/2014_Goodfellow.pdf:PDF},
  groups        = {included},
  keywords      = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {stat.ML},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/1406.2661#},
}

@Misc{2018_Fu,
  author    = {Fu, Justin and Singh, Avi and Ghosh, Dibya and Yang, Larry and Levine, Sergey},
  title     = {{V}ariational {I}nverse {C}ontrol with {E}vents: {A} {G}eneral {F}ramework for {D}ata-{D}riven {R}eward {D}efinition},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1805.11686},
  file      = {:files/2018_Fu.pdf:PDF},
  groups    = {included},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/1805.11686},
}

@Article{2018_Haarnoja,
  author        = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  title         = {{S}oft {A}ctor-{C}ritic {A}lgorithms and {A}pplications},
  year          = {2018},
  month         = dec,
  abstract      = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1812.05905},
  eprint        = {1812.05905},
  file          = {:files/2018_Haarnoja.pdf:PDF},
  groups        = {included},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/1812.05905},
}

@Misc{2020_Zhu,
  author    = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
  title     = {{T}he {I}ngredients of {R}eal-{W}orld {R}obotic {R}einforcement {L}earning},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2004.12570},
  file      = {:files/2020_Zhu.pdf:PDF},
  groups    = {included},
  keywords  = {Machine Learning (cs.LG), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2004.12570},
}

@Article{2021_Liu,
  author        = {Liu, Nan and Li, Shuang and Du, Yilun and Tenenbaum, Joshua B. and Torralba, Antonio},
  title         = {{L}earning to {C}ompose {V}isual {R}elations},
  year          = {2021},
  month         = nov,
  abstract      = {The visual world around us can be described as a structured set of objects and their associated relations. An image of a room may be conjured given only the description of the underlying objects and their associated relations. While there has been significant work on designing deep neural networks which may compose individual objects together, less work has been done on composing the individual relations between objects. A principal difficulty is that while the placement of objects is mutually independent, their relations are entangled and dependent on each other. To circumvent this issue, existing works primarily compose relations by utilizing a holistic encoder, in the form of text or graphs. In this work, we instead propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully. We further show that decomposition enables our model to effectively understand the underlying relational scene structure. Project page at: https://composevisualrelations.github.io/.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Zero v1.0 Universal},
  doi           = {10.48550/ARXIV.2111.09297},
  eprint        = {2111.09297},
  file          = {:files/2021_Liu.pdf:PDF;:notes/2021_Liu.odt:OpenDocument text},
  groups        = {included},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
  readstatus    = {skimmed},
  url           = {https://arxiv.org/abs/2111.09297},
}

@InCollection{2022_Franceschetti,
  author    = {Andrea Franceschetti and Elisa Tosello and Nicola Castaman and Stefano Ghidoni},
  booktitle = {Lecture Notes in Networks and Systems},
  publisher = {Springer International Publishing},
  title     = {{R}obotic {A}rm {C}ontrol and {T}ask {T}raining {T}hrough {D}eep {R}einforcement {L}earning},
  year      = {2022},
  pages     = {532--550},
  doi       = {10.1007/978-3-030-95892-3_41},
  file      = {:files/2022_Franceschetti.pdf:PDF},
  groups    = {included},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-95892-3_41},
}

@Article{2021_Ramesh,
  author        = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  title         = {{Z}ero-{S}hot {T}ext-to-{I}mage {G}eneration},
  year          = {2021},
  month         = feb,
  abstract      = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2102.12092},
  eprint        = {2102.12092},
  file          = {:files/2021_Ramesh.pdf:PDF},
  groups        = {included},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2102.12092},
}

@Misc{2020_Du,
  author     = {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  title      = {{I}mproved {C}ontrastive {D}ivergence {T}raining of {E}nergy {B}ased {M}odels},
  year       = {2020},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.2012.01316},
  file       = {:files/2020_Du.pdf:PDF;:notes/2020_Du.odt:OpenDocument text},
  keywords   = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher  = {arXiv},
  readstatus = {skimmed},
  url        = {https://arxiv.org/abs/2012.01316},
}

@Misc{2018_Mordatch,
  author     = {Mordatch, Igor},
  title      = {{C}oncept {L}earning with {E}nergy-{B}ased {M}odels},
  year       = {2018},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1811.02486},
  file       = {:files/2018_Mordatch.pdf:PDF;:notes/2018_Mordatch.odt:OpenDocument text},
  keywords   = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher  = {arXiv},
  readstatus = {skimmed},
  url        = {https://arxiv.org/abs/1811.02486},
}

@Book{_Sutton,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  publisher = {A Bradford Book},
  title     = {{R}einforcement {L}earning {A}n {I}ntroduction},
  year      = {2018},
  edition   = {Second},
  isbn      = {9780262039246},
  pages     = {552},
  subtitle  = {An Introduction},
}

@Article{2021_Yu,
  author    = {Ning Yu and Lin Nan and Tao Ku},
  journal   = {Industrial Robot: the international journal of robotics research and application},
  title     = {Robot hand-eye cooperation based on improved inverse reinforcement learning},
  year      = {2021},
  month     = {nov},
  number    = {5},
  pages     = {877--884},
  volume    = {49},
  doi       = {10.1108/ir-09-2021-0208},
  file      = {:files/2021_Yu.pdf:PDF},
  publisher = {Emerald},
  url       = {https://www-emerald-com.ezproxy1.bath.ac.uk/insight/content/doi/10.1108/IR-09-2021-0208/full/html},
}

@InProceedings{2006_Konidaris,
  author    = {George Konidaris and Andrew Barto},
  booktitle = {Proceedings of the 23rd international conference on Machine learning - {ICML} {\textquotesingle}06},
  title     = {Autonomous shaping: knowledge transfer in reinforcement learning},
  year      = {2006},
  publisher = {{ACM} Press},
  doi       = {10.1145/1143844.1143906},
}

@Article{2020_Kleeberger,
  author    = {Kilian Kleeberger and Richard Bormann and Werner Kraus and Marco F. Huber},
  journal   = {Current Robotics Reports},
  title     = {{A} {S}urvey on {L}earning-{B}ased {R}obotic {G}rasping},
  year      = {2020},
  month     = {sep},
  number    = {4},
  pages     = {239--249},
  volume    = {1},
  doi       = {10.1007/s43154-020-00021-6},
  publisher = {Springer Science and Business Media {LLC}},
}

@Misc{2016_Johnson,
  author    = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
  title     = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1612.06890},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/1612.06890},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:included\;0\;0\;0x8a8a8aff\;\;\;;
}
