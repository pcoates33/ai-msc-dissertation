# Attempt to generate a fairly basic image semantic segmentation classifier using a UNET.
# Use the images generated by shapes_with_templates.ShapeBuilder
# These are 480 x 640

from shapes_with_templates import ShapeBuilder
from unet_model import UNET

import multiprocessing as mp
import numpy as np
import torch
import torch.nn as nn
import torchvision
import time
from PIL import Image
from utils import split_greyscale, to_greyscale, calculate_dice_score
from pathlib import Path

model_checkpoint_path = "/Users/petercoates/Development/bath-msc/dissertation/ai-msc-dissertation/code/investigations/segmentation/checkpoints/unet-shape-segmentation.tar"

def log(str):
    time_now = time.strftime('%H:%M:%S')
    # print(f'{time_now}: {str}')
    

if __name__ == "__main__":
    # main function
    log('Started.')

    # device = torch.device('cuda' if torch.has_cuda else 'cpu')
    device = torch.device('mps' if torch.has_mps else 'cpu')

    # create a convnet
    net = UNET(out_channels=4)
    epoch_checkpoint = 0

    # using Cross Entropy 
    # critereon = nn.CrossEntropyLoss()
    # Try different loss function - Binary as we're just looking for one shape at the moment.
    critereon = nn.BCEWithLogitsLoss()
    optimiser = torch.optim.Adam(net.parameters(), lr=0.0001)

    # Load any previously saved model
    if Path(model_checkpoint_path).is_file():
        checkpoint = torch.load(model_checkpoint_path)
        if 'epoch' in checkpoint:
            epoch_checkpoint = checkpoint['epoch']
        print(f"Loading model from checkpoint made after {epoch_checkpoint} epochs")
        net.load_state_dict(checkpoint['model_state_dict'])
        net = net.to(device)
        net.train()
        optimiser.load_state_dict(checkpoint['optimizer_state_dict'])
    else:
        net = net.to(device)

    # Need to transform image to tensor - this changes pixel values from [0,255] to [0,1]
    # The normalize step changes the pixel values from [0,1] to [-1, 1]
    # TODO : see how well it behaves without the normalise
    transformer_rgb = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    # transformer_grey = torchvision.transforms.Compose([
    #     torchvision.transforms.ToTensor(),
    #     torchvision.transforms.Normalize((0.5), (0.5))
    # ])

    # Use separate process to generate the shapes.
    batch_size = 50
    test_batch_size = 10
    mini_batch_size = 5
    q_train = mp.Queue()
    q_test = mp.Queue()
    shape_builder_train = ShapeBuilder(q_train, batch_size)
    shape_builder_train.daemon = True
    shape_builder_train.start()
    shape_builder_test = ShapeBuilder(q_test, test_batch_size)
    shape_builder_test.daemon = True
    shape_builder_test.start()
    
    img_count = 0
    # train the network
    epoch_checkpoint += 1
    last_epoch = epoch_checkpoint + 79
    for epoch in range(epoch_checkpoint, last_epoch):
        running_loss = 0.0

        # wait for the shape_builder to finish, then get the shapes from the queue.
        log('Wait for batch of shape images.')
        batch = q_train.get(timeout=20)
        shape_builder_train.join(timeout=10)
        log('Got batch.')

        # kick off another thread to build more shapes        
        shape_builder_train = ShapeBuilder(q_train, batch_size)
        shape_builder_train.daemon = True
        shape_builder_train.start()
   
        log('Process batch.')
        img_end = 0
        # batch = create_batch(batch_size)
        while img_end < batch_size:
            img_start = img_end
            img_end = min(img_start + mini_batch_size, batch_size)
            shape_images = []
            shape_masks = []
            for img, mask, label in batch[img_start:img_end]:
                shape_images.append(transformer_rgb(img))
                # shape_masks.append(split_greyscale(torch.Tensor(mask), [(25, 75), (75, 125), (125, 175), (175, 225)]))
                # Just take the boxes out of the mask.
                shape_masks.append(split_greyscale(torch.Tensor(mask), [(175, 225), (125, 175), (75, 125), (25, 75)]))

            # Swap from lists to Tensors
            shape_images = torch.stack(shape_images).to(device)
            shape_masks = torch.stack(shape_masks).to(device)

            img_count += img_end - img_start
            
            # zero parameter gradients
            optimiser.zero_grad()

            # forward + backward + optimise
            log('Before Forward.')
            predictions = net.forward(shape_images)
            log('After forward')
            
            loss = critereon(predictions, shape_masks)
            log('After loss function')
            loss.backward()
            log('after backwards.')
            optimiser.step()
            log('after optimiser step')

            running_loss += loss.item()
            del predictions
            del shape_masks
            del shape_images
        
        # See how many we get right
        match = 0
        # inputs = create_batch(batch_size)
        
        batch_for_test = q_test.get(timeout=10)
        shape_builder_test.join(timeout=10)

        # kick off another 
        # TODO : add a check for last time around the loop - we don't need to build another set then.
        shape_builder_test = ShapeBuilder(q_test, test_batch_size)
        shape_builder_test.daemon = True
        shape_builder_test.start()

        log('Before accuracy test.')
        with torch.no_grad():
            matched = 0
            out_of = 0
            img_end = 0
            dice_score = 0
            batches = 0
            # batch = create_batch(batch_size)
            while img_end < test_batch_size:
                img_start = img_end
                img_end = min(img_start + mini_batch_size, test_batch_size)
                test_imgs = []
                test_masks = []
                for img, mask, label in batch_for_test[img_start:img_end]:
                    test_imgs.append(transformer_rgb(img))
                    # test_masks.append(split_greyscale(torch.Tensor(mask), [(25, 75), (75, 125), (125, 175), (175, 225)]))
                    test_masks.append(split_greyscale(torch.Tensor(mask), [(175, 225), (125, 175), (75, 125), (25, 75)]))
                test_imgs = torch.stack(test_imgs).to(device)
                test_masks = torch.stack(test_masks).to(device)

                predictions = net(test_imgs)

                log('Got predictions.')
                # Put them in range 0 to 1
                predictions = torch.sigmoid(predictions)
                # Then force to be 0 or 1 for checking accuracy
                predictions.masked_fill_(predictions.ge(0.5), 1.0)
                predictions.masked_fill_(predictions.lt(0.5), 0.0)

                matched += (predictions == test_masks).sum()
                out_of += torch.numel(predictions)
                dice_score += calculate_dice_score(predictions, test_masks)
                batches += 1

            accuracy = matched*100/out_of   
            if batches > 0:
                dice_score = dice_score / batches
            log('Calculated accuracy')     

        # abs_diff = torch.abs(predictions - test_masks)
        # total_diff = torch.sum(abs_diff)
        # avg_diff = total_diff / predictions.shape[0]
        # avg_diff_per_pixel = avg_diff / (predictions.shape[1] * predictions.shape[2] * predictions.shape[3])
        # closer to zero is better for the difference

        # TODO : save the images...
        img = torchvision.transforms.functional.to_pil_image(batch_for_test[-1][0])
        # # the test_masks and predictions now have multiple channels, so merge them back into greyscale image.

        mask = to_greyscale(test_masks[-1].to('cpu'), [200, 150, 100, 50])
        pred = to_greyscale(predictions[-1].to('cpu'), [200, 150, 100, 50])

        mask = torchvision.transforms.functional.to_pil_image(mask, mode='F')
        mask = mask.convert('L')
        pred = torchvision.transforms.functional.to_pil_image(pred, mode='F')
        pred = pred.convert('L')
        # img.show()
        # mask.show()
        # pred.show()
        imgs_path = "/Users/petercoates/Development/bath-msc/dissertation/ai-msc-dissertation/code/investigations/segmentation/temp_imgs"
        img.save(f"{imgs_path}/img_{img_count}.png","PNG")
        mask.save(f"{imgs_path}/mask_{img_count}.png","PNG")
        pred.save(f"{imgs_path}/pred_{img_count}.png","PNG")

        log('images saved')

        # TODO compare predictions against test_masks.
        # _, predicted = torch.max(outputs.data, 1)
        # match = (predicted == labels).sum().item()
        
        # for img, label in inputs:
        #     actual = label['shape_type_idx']
        #     output = net(transformer(img))
        #     predicted = torch.argmax(output).item()
        #     # If the actual type is over 0.5 then it's the most likely.
        #     if actual == predicted:
        #         match += 1

        del predictions
        del test_masks
        del test_imgs
        # print statistics
        time_now = time.strftime('%H:%M:%S')
        # print(f'{time_now} [epoch:{epoch}] total images = {img_count} : Running loss for last {batch_size} images = {running_loss:.3f}. Test matched {match} of {len(inputs)} ')
        print(f'{time_now} [epoch:{epoch}] total images = {img_count} : unmatched pixels = {out_of - matched}. Accuracy = {accuracy:0.3f}. Dice score={dice_score:0.3f}')
        running_loss = 0.0

        # save every 100 epochs - also save if last run is not a multiple of 100
        if epoch % 10 == 0 or epoch == last_epoch:
            # print(f"saving model after epoch {epoch}")
            torch.save({
                'epoch': epoch,
                'model_state_dict': net.state_dict(),
                'optimizer_state_dict': optimiser.state_dict()
                }, model_checkpoint_path)


    print('Finished training.')
    torch.save(net.state_dict(), './basic_classifier_net_2.pth')
    # terminate any remaining shape builders
    shape_builder_train.join(timeout=10)
    inputs = q_train.get(timeout=10)
    shape_builder_test.join(timeout=10)
    inputs = q_test.get(timeout=10)